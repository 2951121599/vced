# Clip 简介

Clip 模型的本核心思想是希望可以通过文本信息来实现视觉任务的训练，将原本的图像分类问题转换为图文匹配问题，通过这样的转换使得模型精度得到了显著的提升，接下来我们就对 Clip 模型进行简单的介绍。

## 1.原理介绍

### 1.1 CV模型

先从简单的 CV 模型 VGG 说起，VGG 模型将多个块进行堆叠，实现了当时人脸识别 state of the art 的结果。后续一些有关研究神经网络模型层间特征图可视化的工作表明，涉及计算机视觉分类的模型更多的依赖于模型末端的神经层。也就是说，越靠近输出的神经网络层越具有任务相关性。这种结果也同样在 NLP 第三范式：pre-trained + fine tune。语言模型在大规模语料上进行预训练之后，便具有了强大的语义表征能力。实验证明 (bert)，将 PLM 接上后续任务相关神经网络层进行微调，可以在下游任务上达到更好的表现。
同样，在 CV 深度神经网络中，通过可视化的方法，也可以发现，模型最前端的神经网络层倾向于提取一些普遍的、共有的视觉特征，如纹理、边缘等信息。越往后则越倾向于任务相关的特征。也就是说，模型在某种程度上具有了提取普遍特征的能力。  

### 1.2 对比学习

这里先说说对比学习。一般情况下，倘若我们要对猫狗二分类，一个简单的想法就是对其分别打上对应的标签，进行分析学习。但与此同时，我们也可以通过对比学习的方法来实现，简单来说，就是不再将所有的猫（或狗）照片通过神经网络分别映射到对应标签，而是让经过神经网络处理之后原属同一类别的特征向量之间的距离尽可能接近，让原本不属于同一类别的特征向量之间的距离尽可能拉远。这就有点聚类的感觉，但是还是有监督信号的。

### 1.3 NLP模型

下面引出语言模型的介绍。bert，gpt 系列的模型离不开 transformer 模型的发明，自注意力机制更好的捕获了数据间的关系。
bert 模型的预训练任务主要为模拟人的完形填空任务，在这种预训练方法下，模型需要同时关注上下文间的信息，从而推理得出当前位置的 token。另一种非常强的 NLP 模型：gpt，则使用了自回归的方法来训练，也就是说，模型仅可通过当前位置之前的字段来推理当前位置的 token。在这之后，unilm 模型将两种注意力模型联合起来进行训练。encoder-decoder 是 NLP 领域普遍使用的框架，但这块内容与 multi modal 部分关系不大，再次不过多赘述。但是如果我们能够将图像信息和语义信息（或者其他模态的信息）映射到同一空间后，再进行训练，那么在某种程度上，模型可以在多种模态之间建立联系。

### 1.4 多模态模型

clip(contrastive language image pre train) 实现了将语言信息和图像信息联合训练，实现了在下游任务上 zero shot（Zero-shot learning 指的是在没有当前类别的训练样本的情况下，让模型学习到一个映射关系可以将这个样本映射到原有的向量空间，再通过向量空间中距离判断的方式推断出当前没有看过的样本可能会属于哪一类别） 的能力。具体来说，clip 将图像的分类任务转化为了图文匹配的任务，更一步，就是通过将图像信息和语义信息映射到同一多模态语义空间下，再使用对比学习的方法进行训练。具体细节可参考论文《[Learning transferable visual models from natural language supervision](https://arxiv.org/pdf/2103.00020.pdf)》。
